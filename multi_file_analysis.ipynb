{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c068ac",
   "metadata": {},
   "source": [
    "# Multi-File Linguistic Analysis - Dementia Dataset\n",
    "This notebook processes JSON files from the dementia folder and performs linguistic analysis on character memory retellings across different ages. Each file contains a character's stories at different ages along with their deterioration onset age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5198f46",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63f75e",
   "metadata": {},
   "source": [
    "## Load All JSON Files from Dementia Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files_from_folder(folder_path):\n",
    "    \"\"\"Load all JSON files from a specified folder.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                all_data.append(data)\n",
    "                character_name = data.get('name', 'Unknown')\n",
    "                start_deterioration_age = data.get('start_deterioration_age', 'Unknown')\n",
    "                print(f\"Loaded: {os.path.basename(file_path)} - Character: {character_name} - Start deterioration age: {start_deterioration_age}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Load all JSON files from the dementia data folder\n",
    "data_folder = \"data_aug_10/dementia\"\n",
    "all_character_data = load_json_files_from_folder(data_folder)\n",
    "\n",
    "print(f\"\\nLoaded {len(all_character_data)} character files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bf302",
   "metadata": {},
   "source": [
    "## Aggregate Retellings Across All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78021ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stories_with_years_before_diagnostic(all_character_data):\n",
    "    \"\"\"\n",
    "    Combine all stories from all characters and calculate years before diagnostic.\n",
    "    Each story becomes a data point with its years before diagnostic based on individual character's start_deterioration_age.\n",
    "    \"\"\"\n",
    "    all_story_points = []\n",
    "    \n",
    "    for character_data in all_character_data:\n",
    "        character_name = character_data.get('name', 'Unknown')\n",
    "        stories = character_data.get('stories', [])\n",
    "        deterioration_age = character_data.get('start_deterioration_age')\n",
    "        \n",
    "        if deterioration_age is None:\n",
    "            print(f\"Warning: No deterioration age found for {character_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        for story_data in stories:\n",
    "            age = story_data.get('age')\n",
    "            story = story_data.get('story', '')\n",
    "            if age is not None and story:\n",
    "                years_before_diagnostic = -(deterioration_age - age)\n",
    "                all_story_points.append({\n",
    "                    'character': character_name,\n",
    "                    'age': age,\n",
    "                    'deterioration_age': deterioration_age,\n",
    "                    'years_before_diagnostic': years_before_diagnostic,\n",
    "                    'story': story\n",
    "                })\n",
    "    \n",
    "    return all_story_points\n",
    "\n",
    "# Aggregate all stories with years before diagnostic calculation using individual deterioration ages\n",
    "all_story_data = aggregate_stories_with_years_before_diagnostic(all_character_data)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Total story data points: {len(all_story_data)}\")\n",
    "if all_story_data:\n",
    "    years_before_range = [point['years_before_diagnostic'] for point in all_story_data]\n",
    "    print(f\"Years before diagnostic range: {min(years_before_range)} to {max(years_before_range)}\")\n",
    "\n",
    "    # Show sample data points\n",
    "    print(f\"\\nSample data points:\")\n",
    "    for i, point in enumerate(all_story_data[:5]):\n",
    "        print(f\"  {point['character']} - Age {point['age']} (deterioration starts at {point['deterioration_age']}) -> {point['years_before_diagnostic']} years before diagnostic\")\n",
    "        print(f\"    Story length: {len(point['story'])} characters\")\n",
    "\n",
    "    # Show distribution by character\n",
    "    character_counts = {}\n",
    "    for point in all_story_data:\n",
    "        char = point['character']\n",
    "        character_counts[char] = character_counts.get(char, 0) + 1\n",
    "    \n",
    "    print(f\"\\nStory distribution by character:\")\n",
    "    for char, count in character_counts.items():\n",
    "        # Get deterioration age from the original data\n",
    "        deterioration_age = next((data['start_deterioration_age'] for data in all_character_data if data.get('name') == char), 'Unknown')\n",
    "        print(f\"  {char}: {count} stories (deterioration starts at age {deterioration_age})\")\n",
    "else:\n",
    "    print(\"No story data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759a5f3",
   "metadata": {},
   "source": [
    "## Plotting Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_with_regression(story_data, metric_function, label_y: str, title_suffix: str = \"\"):\n",
    "    \"\"\"Plot scatter plot of individual stories with linear regression trend line.\"\"\"\n",
    "    if not story_data:\n",
    "        print(f\"No data available for {label_y}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate metric for each story\n",
    "    x_values = []  # years before diagnostic\n",
    "    y_values = []  # metric values\n",
    "    characters = []\n",
    "    \n",
    "    for story_point in story_data:\n",
    "        metric_value = metric_function(story_point['story'])\n",
    "        x_values.append(story_point['years_before_diagnostic'])\n",
    "        y_values.append(metric_value)\n",
    "        characters.append(story_point['character'])\n",
    "    \n",
    "    if not x_values:\n",
    "        print(f\"No valid data points for {label_y}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create scatter plot with different colors for each character\n",
    "    unique_characters = list(set(characters))\n",
    "    colors = plt.cm.tab10(range(len(unique_characters)))\n",
    "    \n",
    "    for i, character in enumerate(unique_characters):\n",
    "        char_mask = [c == character for c in characters]\n",
    "        char_x = [x for x, mask in zip(x_values, char_mask) if mask]\n",
    "        char_y = [y for y, mask in zip(y_values, char_mask) if mask]\n",
    "        \n",
    "        plt.scatter(char_x, char_y, alpha=0.7, s=60, \n",
    "                   color=colors[i], label=character, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add linear regression trend line\n",
    "    if len(x_values) > 1:\n",
    "        z = np.polyfit(x_values, y_values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(min(x_values), max(x_values), 100)\n",
    "        plt.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2, label='Linear Trend')\n",
    "        \n",
    "        # Calculate and display correlation coefficient and p-value\n",
    "        from scipy import stats\n",
    "        correlation, p_value = stats.pearsonr(x_values, y_values)\n",
    "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        plt.text(0.02, 0.98, f'Correlation: {correlation:.3f} ({significance})\\np-value: {p_value:.4f}', \n",
    "                transform=plt.gca().transAxes, fontsize=10, \n",
    "                verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # Standardize x-axis ticks and limits\n",
    "    plt.xlim(-16, 10)\n",
    "    plt.xticks(range(-15, 10, 3))\n",
    "    plt.xlabel('Years Before Diagnostic', fontsize=12)\n",
    "    plt.ylabel(f'{label_y}', fontsize=12)\n",
    "    plt.title(f'{label_y} vs Years Before Diagnostic{title_suffix}', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metric_for_all_stories(story_data, metric_function):\n",
    "    \"\"\"Calculate metric for each individual story.\"\"\"\n",
    "    story_metrics = []\n",
    "    \n",
    "    for story_point in story_data:\n",
    "        metric_value = metric_function(story_point['story'])\n",
    "        story_metrics.append({\n",
    "            'character': story_point['character'],\n",
    "            'age': story_point['age'],\n",
    "            'years_before_diagnostic': story_point['years_before_diagnostic'],\n",
    "            'metric_value': metric_value,\n",
    "            'story': story_point['story']\n",
    "        })\n",
    "    \n",
    "    return story_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8780cd",
   "metadata": {},
   "source": [
    "## Count Nouns by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea286a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_freq_in_text(text):\n",
    "    \"\"\"Count the number of noun phrases in the given text.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    return len(blob.noun_phrases) / len(tokens)\n",
    "\n",
    "# Plot noun frequency scatter plot\n",
    "plot_scatter_with_regression(all_story_data, nouns_freq_in_text, \"Noun Phrase Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907faea",
   "metadata": {},
   "source": [
    "## Calculate Hapax Legomena Frequency by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_used_once_frequency(text):\n",
    "    \"\"\"Calculate the frequency of words used only once in the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    once_count = sum(1 for _, count in word_freq.items() if count == 1)\n",
    "    return once_count / len(tokens)\n",
    "\n",
    "# Plot hapax legomena frequency scatter plot\n",
    "plot_scatter_with_regression(all_story_data, word_used_once_frequency, \"Hapax Legomena Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e667984",
   "metadata": {},
   "source": [
    "## Calculate Words Used Once or Twice Frequency by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_used_once_or_twice_frequency(text):\n",
    "    \"\"\"Calculate the frequency of words used once or twice in the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    once_or_twice_count = sum(1 for _, count in word_freq.items() if count <= 2)\n",
    "    return once_or_twice_count / len(tokens)\n",
    "\n",
    "# Plot words used once or twice frequency scatter plot\n",
    "plot_scatter_with_regression(all_story_data, word_used_once_or_twice_frequency, \"Words Used Once or Twice Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279075f7",
   "metadata": {},
   "source": [
    "## Calculate Brunet Index by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brunet_index(text, alpha=-0.165):\n",
    "    \"\"\"Calculate the Brunet index for the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    unique_words = len(word_freq)\n",
    "    tokens_count = len(tokens)\n",
    "    if unique_words == 0 or tokens_count == 0:\n",
    "        return 0\n",
    "    return tokens_count ** (unique_words ** alpha)\n",
    "\n",
    "# Plot Brunet index scatter plot\n",
    "plot_scatter_with_regression(all_story_data, brunet_index, \"Brunet Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3cfb3",
   "metadata": {},
   "source": [
    "## Calculate Type-Token Ratio by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae519d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_type_ratio(text):\n",
    "    \"\"\"Calculate the type-token ratio for the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    unique_words = len(word_freq)\n",
    "    tokens_count = len(tokens)\n",
    "    return unique_words / tokens_count if tokens_count > 0 else 0\n",
    "\n",
    "# Plot type-token ratio scatter plot\n",
    "plot_scatter_with_regression(all_story_data, token_type_ratio, \"Type-Token Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4201b441",
   "metadata": {},
   "source": [
    "## Calculate Adposition Frequency by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adposition_frequency(text):\n",
    "    \"\"\"Calculate the frequency of adpositions in the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    words_and_tags = nltk.pos_tag(tokens)\n",
    "    adpositions_count = sum(1 for _, tag in words_and_tags if tag == \"IN\")\n",
    "    return adpositions_count / len(tokens)\n",
    "\n",
    "# Plot adposition frequency scatter plot\n",
    "plot_scatter_with_regression(all_story_data, adposition_frequency, \"Adposition Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f057a",
   "metadata": {},
   "source": [
    "## Calculate Unigram and Bigram Repetitions by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_and_bi_grams_repetitions(text):\n",
    "    \"\"\"Calculate the number of unique unigrams and bigrams in the text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    unigrams = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    return len(unigrams) + len(bigrams)\n",
    "\n",
    "# Plot unigram and bigram repetitions scatter plot\n",
    "plot_scatter_with_regression(all_story_data, uni_and_bi_grams_repetitions, \"Unigram and Bigram Repetitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ae9df",
   "metadata": {},
   "source": [
    "## Load Word Frequency Data (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cf723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    \"\"\"Load a CSV file and return its content.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Load word frequency data\n",
    "try:\n",
    "    subtl_and_zipf_df = load_csv('subtl_and_zipf.csv')\n",
    "    # Filter relevant columns\n",
    "    subtl_and_zipf_df = subtl_and_zipf_df[['Word','FREQcount', 'SUBTLWF', 'Zipf-value']]\n",
    "    print(\"Successfully loaded word frequency data.\")\n",
    "    print(f\"Shape: {subtl_and_zipf_df.shape}\")\n",
    "    print(subtl_and_zipf_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: subtl_and_zipf.csv not found. Skipping frequency analysis.\")\n",
    "    subtl_and_zipf_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62608e52",
   "metadata": {},
   "source": [
    "## Calculate SUBTLEX Frequency by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_subtl(text, corpus_data):\n",
    "    \"\"\"Calculate the frequency of words in the text based on SUBTLEXus data.\"\"\"\n",
    "    if corpus_data is None:\n",
    "        return 0\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    \n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    total_freq = sum(word_freq.values())\n",
    "    \n",
    "    subtl_freq = 0\n",
    "    for word, freq in word_freq.items():\n",
    "        word_lower = word.lower()\n",
    "        matching_rows = corpus_data[corpus_data['Word'] == word_lower]\n",
    "        if not matching_rows.empty:\n",
    "            subtl_value = matching_rows['SUBTLWF'].values[0]\n",
    "            subtl_freq += freq * subtl_value\n",
    "    \n",
    "    return subtl_freq / total_freq if total_freq > 0 else 0\n",
    "\n",
    "# Plot SUBTLEX frequency scatter plot\n",
    "if subtl_and_zipf_df is not None:\n",
    "    def subtl_metric(text):\n",
    "        return word_freq_subtl(text, subtl_and_zipf_df)\n",
    "    \n",
    "    plot_scatter_with_regression(all_story_data, subtl_metric, \"SUBTLEX Frequency\")\n",
    "else:\n",
    "    print(\"Skipping SUBTLEX frequency analysis due to missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5706ff",
   "metadata": {},
   "source": [
    "## Calculate Zipf Frequency by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f041fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_zipf(text, corpus_data):\n",
    "    \"\"\"Calculate the frequency of words in the text based on Zipf data.\"\"\"\n",
    "    if corpus_data is None:\n",
    "        return 0\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    \n",
    "    word_freq = nltk.FreqDist(tokens)\n",
    "    total_freq = sum(word_freq.values())\n",
    "    \n",
    "    zipf_freq = 0\n",
    "    for word, freq in word_freq.items():\n",
    "        word_lower = word.lower()\n",
    "        matching_rows = corpus_data[corpus_data['Word'] == word_lower]\n",
    "        if not matching_rows.empty:\n",
    "            zipf_value = matching_rows['Zipf-value'].values[0]\n",
    "            zipf_freq += freq * zipf_value\n",
    "    \n",
    "    return zipf_freq / total_freq if total_freq > 0 else 0\n",
    "\n",
    "# Plot Zipf frequency scatter plot\n",
    "if subtl_and_zipf_df is not None:\n",
    "    def zipf_metric(text):\n",
    "        return word_freq_zipf(text, subtl_and_zipf_df)\n",
    "    \n",
    "    plot_scatter_with_regression(all_story_data, zipf_metric, \"Zipf Frequency\")\n",
    "else:\n",
    "    print(\"Skipping Zipf frequency analysis due to missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17838eee",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization - All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf01972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_scatter_plot():\n",
    "    \"\"\"Create a comprehensive 3x3 subplot showing all linguistic metrics as scatter plots.\"\"\"\n",
    "    \n",
    "    # Prepare the metric functions\n",
    "    metric_functions = [\n",
    "        (nouns_freq_in_text, \"Noun Phrases\", \"Frequency\"),\n",
    "        (word_used_once_frequency, \"Hapax Legomena\", \"Frequency\"),\n",
    "        (word_used_once_or_twice_frequency, \"Words Used ≤2 Times\", \"Frequency\"),\n",
    "        (brunet_index, \"Brunet Index\", \"Index Value\"),\n",
    "        (token_type_ratio, \"Type-Token Ratio\", \"Ratio\"),\n",
    "        (adposition_frequency, \"Adposition Frequency\", \"Frequency\"),\n",
    "        (uni_and_bi_grams_repetitions, \"Unigram & Bigram\", \"Count\"),\n",
    "    ]\n",
    "    \n",
    "    # Add frequency metrics if available\n",
    "    if subtl_and_zipf_df is not None:\n",
    "        def zipf_metric(text):\n",
    "            return word_freq_zipf(text, subtl_and_zipf_df)\n",
    "        def subtl_metric(text):\n",
    "            return word_freq_subtl(text, subtl_and_zipf_df)\n",
    "        \n",
    "        uni_gram_and_bigram = metric_functions[-1]\n",
    "        metric_functions[-1] = (zipf_metric, \"Zipf Frequency\", \"Frequency\")\n",
    "        metric_functions.append((subtl_metric, \"SUBTLEX Frequency\", \"Frequency\"))\n",
    "        metric_functions.append(uni_gram_and_bigram)\n",
    "    \n",
    "    # Create the subplot\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 16))\n",
    "    fig.suptitle('Comprehensive Linguistic Analysis: Individual Stories vs Years Before Diagnostic', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get unique characters and colors\n",
    "    all_characters = list(set(point['character'] for point in all_story_data))\n",
    "    colors = plt.cm.tab10(range(len(all_characters)))\n",
    "    character_colors = dict(zip(all_characters, colors))\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, (metric_function, title, ylabel) in enumerate(metric_functions):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Calculate metric for all stories\n",
    "        x_values = []  # years before diagnostic\n",
    "        y_values = []  # metric values\n",
    "        characters = []\n",
    "        \n",
    "        for story_point in all_story_data:\n",
    "            try:\n",
    "                metric_value = metric_function(story_point['story'])\n",
    "                x_values.append(story_point['years_before_diagnostic'])\n",
    "                y_values.append(metric_value)\n",
    "                characters.append(story_point['character'])\n",
    "            except:\n",
    "                continue  # Skip if metric calculation fails\n",
    "        \n",
    "        if x_values:\n",
    "            # Create scatter plot with character-specific colors\n",
    "            for character in all_characters:\n",
    "                char_mask = [c == character for c in characters]\n",
    "                char_x = [x for x, mask in zip(x_values, char_mask) if mask]\n",
    "                char_y = [y for y, mask in zip(y_values, char_mask) if mask]\n",
    "                \n",
    "                if char_x:  # Only plot if there are data points for this character\n",
    "                    ax.scatter(char_x, char_y, alpha=0.6, s=40, \n",
    "                             color=character_colors[character], label=character, \n",
    "                             edgecolors='black', linewidth=0.3)\n",
    "            \n",
    "            # Add linear regression trend line\n",
    "            if len(x_values) > 1:\n",
    "                try:\n",
    "                    z = np.polyfit(x_values, y_values, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_trend = np.linspace(min(x_values), max(x_values), 100)\n",
    "                    ax.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=1.5)\n",
    "                    \n",
    "                    # Calculate correlation\n",
    "                    correlation = np.corrcoef(x_values, y_values)[0, 1]\n",
    "                    ax.text(0.02, 0.98, f'r={correlation:.3f}', \n",
    "                           transform=ax.transAxes, fontsize=8, \n",
    "                           verticalalignment='top', \n",
    "                           bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.7))\n",
    "                except:\n",
    "                    pass  # Skip trend line if calculation fails\n",
    "            \n",
    "            # Customize the subplot\n",
    "            ax.set_xlabel('Years Before Diagnostic', fontsize=9)\n",
    "            ax.set_ylabel(ylabel, fontsize=9)\n",
    "            ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.set_xticks(np.arange(3, max(x_values)+1, 3))\n",
    "            ax.set_xlim(left=0)\n",
    "\n",
    "        else:\n",
    "            # Show message for missing data\n",
    "            ax.text(0.5, 0.5, 'No Data Available', \n",
    "                   ha='center', va='center', transform=ax.transAxes, \n",
    "                   fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "            ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "            ax.set_xlabel('Years Before Diagnostic', fontsize=9)\n",
    "            ax.set_ylabel(ylabel, fontsize=9)\n",
    "    \n",
    "    # Add a single legend for all subplots\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig.legend(handles, labels, loc='center right', bbox_to_anchor=(0.98, 0.5), \n",
    "                  title='Characters', title_fontsize=10, fontsize=8)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 0.85, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Create the comprehensive scatter plot visualization\n",
    "create_comprehensive_scatter_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
